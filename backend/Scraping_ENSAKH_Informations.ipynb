{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e512f235",
   "metadata": {},
   "source": [
    "# Importation des biblioth√®ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cb5e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ec3d10",
   "metadata": {},
   "source": [
    "# D√©partements et fili√®res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21de6547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Traitement du d√©partement : Math√©matiques et Informatique\n",
      "üîé Traitement du d√©partement : G√©nie Electrique\n",
      "üîé Traitement du d√©partement : Cybers√©curit√© R√©seaux et T√©l√©communications\n",
      "üîé Traitement du d√©partement : G√©nie des Proc√©d√©s\n",
      "\n",
      "‚úÖ Donn√©es export√©es dans le fichier JSON : 'departements_ENSAKH.json'\n"
     ]
    }
   ],
   "source": [
    "def get_department_links(BASE_URL=\"http://ensak.usms.ac.ma\"):\n",
    "    url = f\"{BASE_URL}/ensak/departements/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    departments = []\n",
    "\n",
    "    divs = soup.select(\"div.col-sm-6\")\n",
    "    for div in divs:\n",
    "        a_tag = div.find(\"a\")\n",
    "        if a_tag:\n",
    "            name = a_tag.find(\"button\").text.strip() if a_tag.find(\"button\") else \"Nom non trouv√©\"\n",
    "            link = a_tag[\"href\"]\n",
    "            if not link.startswith(\"http\"):\n",
    "                link = BASE_URL + link\n",
    "            departments.append({\"Nom\": name, \"Lien\": link})\n",
    "    return departments\n",
    "\n",
    "\n",
    "def extract_department_info(dept):\n",
    "    url = dept[\"Lien\"]\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    chef = \"Non trouv√©\"\n",
    "    adjoint = \"Non trouv√©\"\n",
    "    filieres = []\n",
    "    professeurs = []\n",
    "\n",
    "    tables = soup.find_all(\"table\")\n",
    "    if tables:\n",
    "        # Table 1: Chef et adjoint\n",
    "        rows = tables[0].find_all(\"tr\")\n",
    "        for row in rows:\n",
    "            cols = row.find_all(\"td\")\n",
    "            if len(cols) >= 2:\n",
    "                key = cols[0].get_text(strip=True)\n",
    "                value = cols[1].get_text(strip=True)\n",
    "                if \"Chef de d√©partement\" in key:\n",
    "                    chef = value if value != \"-\" else \"N'existe pas\"\n",
    "                elif \"Adjoint\" in key:\n",
    "                    adjoint = value if value != \"-\" else \"N'existe pas\"\n",
    "\n",
    "        # Table 2: Professeurs\n",
    "        if len(tables) > 1:\n",
    "            rows = tables[1].find_all(\"tr\")[1:]  # Ignorer l'en-t√™te\n",
    "            for row in rows:\n",
    "                cols = row.find_all(\"td\")\n",
    "                if len(cols) >= 2:\n",
    "                    nom = cols[0].get_text(strip=True)\n",
    "                    email_tag = cols[1].find(\"a\")\n",
    "                    email = email_tag.get_text(strip=True) if email_tag else \"Email non trouv√©\"\n",
    "                    professeurs.append({\"Nom\": nom, \"Email\": email})\n",
    "\n",
    "    # Extraction des fili√®res\n",
    "    h4_tags = soup.find_all(\"h4\")\n",
    "    for h4 in h4_tags:\n",
    "        filiere = h4.get_text(strip=True)\n",
    "        if filiere:\n",
    "            filieres.append(filiere)\n",
    "\n",
    "    return {\n",
    "        \"Nom D√©partement\": dept[\"Nom\"],\n",
    "        \"Lien\": dept[\"Lien\"],\n",
    "        \"Chef\": chef,\n",
    "        \"Adjoint\": adjoint,\n",
    "        \"Fili√®res\": filieres if filieres else [\"Aucune fili√®re\"],\n",
    "        \"Professeurs\": professeurs\n",
    "    }\n",
    "\n",
    "\n",
    "def main(fichier_json=\"departements_ENSAKH.json\"):\n",
    "    departments = get_department_links()\n",
    "    all_data = []\n",
    "\n",
    "    for dept in departments:\n",
    "        print(f\"üîé Traitement du d√©partement : {dept['Nom']}\")\n",
    "        info = extract_department_info(dept)\n",
    "        all_data.append(info)\n",
    "        time.sleep(1)\n",
    "\n",
    "    with open(fichier_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(all_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"\\n‚úÖ Donn√©es export√©es dans le fichier JSON : '{fichier_json}'\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035b6beb",
   "metadata": {},
   "source": [
    "# Ensa en chiffres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aabf9cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_all_counters(url=\"http://ensak.usms.ac.ma/ensak/\", output_file='chiffres_ENSAKH.json'):\n",
    "    # Obtenir le contenu HTML depuis l'URL\n",
    "    response = requests.get(url)\n",
    "    response.encoding = 'utf-8'  # Important pour bien g√©rer les accents\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Erreur lors de la requ√™te HTTP : {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    data = []\n",
    "\n",
    "    counter_blocks = soup.find_all('div', class_='wpsm_counterbox')\n",
    "\n",
    "    for block in counter_blocks:\n",
    "        number_tag = block.find('span', class_='counter')\n",
    "        title_tag = block.find('h3', class_='wpsm_count-title')\n",
    "\n",
    "        number = number_tag.text.strip() if number_tag else None\n",
    "        title = title_tag.text.strip() if title_tag else None\n",
    "\n",
    "        data.append({'Nombre': number, 'Titre': title})\n",
    "\n",
    "    # Enregistrer dans un fichier JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Exemple d'ex√©cution\n",
    "extract_all_counters()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4356f19",
   "metadata": {},
   "source": [
    "# Clubs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fef78663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Les informations des clubs ont √©t√© enregistr√©es dans 'clubs_ENSAKH.json'.\n"
     ]
    }
   ],
   "source": [
    "def extraire_clubs_ensak(url=\"http://ensak.usms.ac.ma/ensak/\"):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Erreur lors de la requ√™te HTTP : {e}\")\n",
    "        return\n",
    "\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    clubs_data = []\n",
    "\n",
    "    # S√©lectionner tous les blocs de clubs\n",
    "    club_blocks = soup.select(\"div.col-xs-12.zoom.campus\")\n",
    "\n",
    "    for block in club_blocks:\n",
    "        # Extraire le lien vers la page du club\n",
    "        link_tag = block.find(\"a\", class_=\"img-thumb\")\n",
    "        club_link = link_tag[\"href\"] if link_tag and \"href\" in link_tag.attrs else None\n",
    "\n",
    "        # Extraire le nom du club\n",
    "        title_tag = block.find(\"h3\")\n",
    "        club_name = title_tag.get_text(strip=True) if title_tag else \"Nom non trouv√©\"\n",
    "\n",
    "        # Initialiser le r√©sum√©\n",
    "        resume = \"R√©sum√© non trouv√©\"\n",
    "\n",
    "        # Acc√©der √† la page du club pour extraire le r√©sum√©\n",
    "        if club_link:\n",
    "            try:\n",
    "                detail_response = requests.get(club_link)\n",
    "                detail_response.raise_for_status()\n",
    "                detail_soup = BeautifulSoup(detail_response.content, \"html.parser\")\n",
    "\n",
    "                # Extraire le contenu de la section 'courses-info'\n",
    "                content_divs = detail_soup.find_all(\"div\", class_=\"courses-info\")\n",
    "                if len(content_divs) >= 3:\n",
    "                    content_div = content_divs[2]  # la troisi√®me div\n",
    "                else:\n",
    "                    content_div = None\n",
    "                if content_div:\n",
    "                    # Supprimer les images et les tableaux\n",
    "                    for tag in content_div.find_all([\"img\", \"table\"]):\n",
    "                        tag.decompose()\n",
    "                    # Extraire le texte\n",
    "                    resume = content_div.get_text(separator=\"\\n\", strip=True)\n",
    "            except requests.RequestException as e:\n",
    "                resume = f\"Erreur lors de l'acc√®s au r√©sum√© : {e}\"\n",
    "\n",
    "            # Pause pour √©viter de surcharger le serveur\n",
    "            time.sleep(1)\n",
    "\n",
    "        clubs_data.append({\n",
    "            \"Nom du Club\": club_name,\n",
    "            \"Lien\": club_link,\n",
    "            \"R√©sum√©\": resume\n",
    "        })\n",
    "\n",
    "    # Enregistrer dans un fichier JSON\n",
    "    with open(\"clubs_ENSAKH.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(clubs_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(\"‚úÖ Les informations des clubs ont √©t√© enregistr√©es dans 'clubs_ENSAKH.json'.\")\n",
    "\n",
    "# Ex√©cuter la fonction\n",
    "extraire_clubs_ensak()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac67218",
   "metadata": {},
   "source": [
    "# les professeurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "def2a032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Les informations des enseignants ont √©t√© enregistr√©es dans 'enseignants_ENSAKH.json'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def extraire_enseignants(url=\"http://ensak.usms.ac.ma/ensak/our-instructors/\", fichier_json=\"enseignants_ENSAKH.json\"):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "        prof_blocks = soup.select(\"div.col-xs-12.col-sm-4.teachers\")\n",
    "        profs_data = []\n",
    "\n",
    "        for prof in prof_blocks:\n",
    "            # Nom et lien\n",
    "            name_tag = prof.select_one(\"h4.author-name a\")\n",
    "            nom = name_tag.text.strip() if name_tag else \"Nom non trouv√©\"\n",
    "            lien = name_tag[\"href\"] if name_tag else \"#\"\n",
    "\n",
    "            # Fonction / titre\n",
    "            p_tags = prof.select(\"p\")\n",
    "            fonction = p_tags[0].text.strip() if len(p_tags) > 0 else \"Fonction non trouv√©e\"\n",
    "\n",
    "            # Email\n",
    "            email_tag = prof.select_one(\"p.email a\")\n",
    "            email = email_tag.text.strip() if email_tag else \"Email non trouv√©\"\n",
    "\n",
    "            # Description d√©taill√©e depuis la page du prof\n",
    "            description = \"Description non trouv√©e\"\n",
    "            if lien != \"#\":\n",
    "                try:\n",
    "                    detail_response = requests.get(lien)\n",
    "                    detail_soup = BeautifulSoup(detail_response.content, \"html.parser\")\n",
    "                    divs = detail_soup.select(\"section.co-author > div\")\n",
    "                    if len(divs) >= 3:\n",
    "                        third_div = divs[2]  # le troisi√®me div\n",
    "                        p_tags_detail = third_div.find_all(\"p\")\n",
    "                        if len(p_tags_detail) >= 2:\n",
    "                            description = p_tags_detail[1].get_text(strip=True)\n",
    "                        else:\n",
    "                            description = \"Deuxi√®me paragraphe non trouv√© dans le 3e div\"\n",
    "                    else:\n",
    "                        description = \"Troisi√®me div non trouv√©\"\n",
    "                except Exception as e:\n",
    "                    description = f\"Erreur lors de l'acc√®s √† la page prof : {e}\"\n",
    "\n",
    "                time.sleep(1)  # Pause pour √©viter surcharge\n",
    "\n",
    "            profs_data.append({\n",
    "                \"Nom\": nom,\n",
    "                \"Lien\": lien,\n",
    "                \"Fonction\": fonction,\n",
    "                \"Email\": email,\n",
    "                \"Description\": description\n",
    "            })\n",
    "\n",
    "        # Sauvegarde en JSON\n",
    "        with open(fichier_json, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(profs_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "        print(f\"‚úÖ Les informations des enseignants ont √©t√© enregistr√©es dans '{fichier_json}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lors de l'extraction : {e}\")\n",
    "\n",
    "# Lancer la fonction\n",
    "extraire_enseignants()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f003185",
   "metadata": {},
   "source": [
    "# formations: double diplomation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "738898a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Donn√©es regroup√©es export√©es dans 'ecoles_modalites_DD_ENSAKH.json' au format liste d'objets.\n"
     ]
    }
   ],
   "source": [
    "def scrape_modalites_ecoles_grouped_list(url=\"http://ensak.usms.ac.ma/ensak/double-diplomation/\", json_filename=\"ecoles_modalites_DD_ENSAKH.json\"):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    paragraph = soup.find(\"p\", style=\"margin-left: 30px;\")\n",
    "    if not paragraph:\n",
    "        print(\"‚ùå Paragraphe non trouv√©.\")\n",
    "        return []\n",
    "\n",
    "    lines = [str(item).strip() for item in paragraph.decode_contents().split(\"<br/>\") if item.strip()]\n",
    "    data = {}\n",
    "\n",
    "    for line in lines:\n",
    "        soup_line = BeautifulSoup(line, \"html.parser\")\n",
    "        i_tag = soup_line.find(\"i\", class_=\"fa fa-arrow-right\")\n",
    "\n",
    "        if not i_tag:\n",
    "            continue\n",
    "\n",
    "        school_name = i_tag.previous_sibling.strip().lstrip(\"-\").strip()\n",
    "        modalities_text = i_tag.next_sibling.strip().rstrip(\".\")\n",
    "        modalities_clean = [m.strip() for m in modalities_text.replace(\"et\", \",\").split(\",\") if m.strip()]\n",
    "\n",
    "        if school_name not in data:\n",
    "            data[school_name] = []\n",
    "        data[school_name].extend(modalities_clean)\n",
    "\n",
    "    # Ajout sp√©cial POLYTECH Angers\n",
    "    if \"POLYTECH Angers\" not in data:\n",
    "        data[\"POLYTECH Angers\"] = []\n",
    "    data[\"POLYTECH Angers\"].extend([\"Double diplomation\", \"Master\"])\n",
    "\n",
    "    # Supprimer doublons et trier\n",
    "    for school in data:\n",
    "        data[school] = sorted(list(set(data[school])))\n",
    "\n",
    "    # Transformer en liste de dicts avec cl√© \"√âcole\" et \"Modalit√©s\"\n",
    "    data_list = [{\"√âcole\": school, \"Modalit√©s\": modalities} for school, modalities in sorted(data.items())]\n",
    "\n",
    "    # Sauvegarde JSON\n",
    "    with open(json_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_list, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"‚úÖ Donn√©es regroup√©es export√©es dans '{json_filename}' au format liste d'objets.\")\n",
    "\n",
    "    \n",
    "\n",
    "# Test\n",
    "scrape_modalites_ecoles_grouped_list()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
